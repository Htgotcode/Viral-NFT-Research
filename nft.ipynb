{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "#nltk.download() #only for the first time running it\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import regex as re\n",
    "from textblob import TextBlob\n",
    "import numpy\n",
    "\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "output['created_at'] =  pd.to_datetime(output['created_at'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "output = output.drop(['author_id', 'text', 'retweet_count', 'quote_count', 'reply_count', 'like_count'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#month data\n",
    "from matplotlib.dates import DateFormatter\n",
    "# time_df = output\n",
    "# time_df.set_index('created_at', inplace=True)\n",
    "# time_df_day_avg = time_df.resample('1H').mean()\n",
    "time_df.groupby([time_df.index.hour]).mean()\n",
    "# fig, ax = plt.subplots(figsize=(12, 12))\n",
    "# ax.bar(time_df_day_avg.index.values,\n",
    "#        time_df_day_avg['total'],\n",
    "#        color='purple')\n",
    "# ax.set(xlabel=\"Day\",\n",
    "#        ylabel=\"Average Total Interactions\",\n",
    "#        title=\"Daily Average Total Interactions\")\n",
    "# date_form = DateFormatter(\"%B %d\")\n",
    "# ax.xaxis.set_major_formatter(date_form)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def createDataFrame():  \n",
    "    with open(\"C:/Users/Administrator/Desktop/tweets.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "\n",
    "            try:\n",
    "                yield (row['author_id'],\n",
    "                        row['text'],\n",
    "                        row['public_metrics']['retweet_count'],\n",
    "                        row['public_metrics']['quote_count'],\n",
    "                        row['public_metrics']['reply_count'],\n",
    "                        row['public_metrics']['like_count'],\n",
    "                        row['created_at'])\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "#create csv\n",
    "df = pd.DataFrame(createDataFrame())\n",
    "df.columns =['author_id', 'text',  'retweet_count', 'quote_count', 'reply_count', 'like_count', 'created_at']\n",
    "df.to_json('datain/nft_tweets.jsonl', orient='records', index=True, lines= True)\n",
    "output = pd.read_json(\"datain/nft_tweets.jsonl\", lines = True)\n",
    "output['total'] = output[['retweet_count', 'quote_count','reply_count', 'like_count']].sum(axis=1)\n",
    "#sort by highest total\n",
    "output = output.sort_values(by = 'total', ascending = False)\n",
    "#top 100\n",
    "# output = output.head(100)\n",
    "# output.to_json('datain/all_data.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "# data = pd.read_json(\"C:/Users/Administrator/Desktop/tweets.jsonl\", lines=True)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data = pd.read_json(\"datain/all_data.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('rt')\n",
    "stop_words.append('nft')\n",
    "# stop_words.append('#nft')\n",
    "\n",
    "#function for cleaning a tweet (remove mentions, hashtags, links, html entities, stop words. And make sure it's only letters)\n",
    "def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        tweet = str.lower(tweet)\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "        tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|(#[A-Za-z0-9_]+)\", \" \", tweet).split()) # remove mentions and hashtags\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
    "        tweet = re.sub(\"(http\\S+|http)\", \"\", tweet, flags=re.MULTILINE) # remove links\n",
    "        tweet = re.sub('\\&\\w+', \"\", tweet) # remove html entities\n",
    "        tweet = re.sub('[^a-zA-Z# ]+', ' ', tweet) # make sure tweet is only letters\n",
    "        # stem & remove stop words\n",
    "        # tweet = ' '.join([PorterStemmer().stem(word=word) for word in tweet.split() if word not in stop_words])\n",
    "        tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "        return tweet\n",
    "\n",
    "#clean data\n",
    "for i in all_data.index:\n",
    "    text = all_data[\"text\"][i]\n",
    "    cleaned_text = clean_tweet(text)\n",
    "    cleaned_text = html.unescape(cleaned_text)\n",
    "    all_data[\"text\"][i] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_json('datain/nft_search_tweets_sample_cleaned.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = sample_data['text'].str.cat(sep=' ')\n",
    "tokens = nltk.word_tokenize(tweet_text)\n",
    "most_common = pd.DataFrame(nltk.ngrams(tokens, 1)).value_counts().to_frame()\n",
    "# terms_count = term_data['text'].value_counts().to_dict()\n",
    "# terms_count = pd.DataFrame.from_dict(terms_count, orient='index')\n",
    "\n",
    "# terms_count.to_html(\"dataout/terms_count.html\")\n",
    "\n",
    "most_common.to_html('dataout/term_freq.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nft_top_100_sentiment <- Has interactions and sentiment totals\n",
    "positivity = ''\n",
    "output = pd.read_json(\"datain/all_data.jsonl\", lines = True)\n",
    "def getSentiment():\n",
    "    for i in output.index:\n",
    "        row = TextBlob(output.iloc[i]['text'])\n",
    "        if row.sentiment.polarity >= 0.7:\n",
    "                positivity = 'mostly_positive'\n",
    "        elif row.sentiment.polarity <= -0.7:\n",
    "            positivity = 'mostly_negative'\n",
    "        elif row.sentiment.polarity > -0.7 and row.sentiment.polarity < -0.4:\n",
    "            positivity = 'negative'\n",
    "        elif row.sentiment.polarity > 0.4 and row.sentiment.polarity < 0.7:\n",
    "                positivity = 'positive'\n",
    "        else:\n",
    "            positivity = 'nuetral'\n",
    "        yield  row.sentiment.polarity, row.sentiment.subjectivity, positivity\n",
    "        \n",
    "df = pd.DataFrame(getSentiment())\n",
    "df.columns =['polarity', 'subjectivity', 'positivity']\n",
    "output['polarity'] = df['polarity']\n",
    "output['subjectivity'] = df['subjectivity']\n",
    "output['positivity'] = df['positivity']\n",
    "output.to_json('datain/nft_cleaned_interactions_sentiment.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud of top 100 NFT's\n",
    "top_100_word_cloud = pd.read_json(\"datain/nft_top_100_cleaned_interactions_sentiment.jsonl\", lines=True)\n",
    "# Generate a word cloud image\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update((\"t\", \"co\", \"https\", \"t\", \"amp\", \"RT\"))\n",
    "wordcloud = WordCloud(stopwords=stopwords,background_color='white', max_words=1000,contour_color='#023075',contour_width=3,colormap='rainbow').generate(' '.join(top_100_word_cloud['text']))\n",
    "# create image as cloud\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# store to file\n",
    "plt.savefig(\"cloud.png\", format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment sentiment\n",
    "#### Read from nft_search_tweets_sample - need replied to with text and author\n",
    "#### Original Poster\n",
    "def createOriginalDataFrame():  \n",
    "    with open(\"datain/nft_search_tweets_sample.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                if row[\"lang\"] == 'en'and len(row.get('referenced_tweets', [])) == 0:\n",
    "                    yield (row['id'],\n",
    "                            row['text'],\n",
    "                            row['author_id'],\n",
    "                            row['public_metrics']['retweet_count'],\n",
    "                            row['public_metrics']['quote_count'],\n",
    "                            row['public_metrics']['reply_count'],\n",
    "                            row['public_metrics']['like_count'],\n",
    "                            row['attachments']['media_keys'],\n",
    "                            row['created_at']\n",
    "                    )\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "def createRepliesDataFrame():  \n",
    "    with open(\"datain/nft_search_tweets_sample.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                if row[\"lang\"] == 'en'and len(row.get('referenced_tweets', [])) != 0:\n",
    "                    yield (row['id'],\n",
    "                            row['text'],\n",
    "                            row['author_id'],\n",
    "                            row['in_reply_to_user_id'],\n",
    "                            row['public_metrics']['retweet_count'],\n",
    "                            row['public_metrics']['quote_count'],\n",
    "                            row['public_metrics']['reply_count'],\n",
    "                            row['public_metrics']['like_count'],\n",
    "                            row['attachments']['media_keys'],\n",
    "                            row['created_at']\n",
    "                    )\n",
    "            except KeyError:\n",
    "                pass\n",
    "#create csv\n",
    "original_data = pd.DataFrame(createOriginalDataFrame())\n",
    "original_data.columns =['id', 'text', 'in_reply_to_user_id', 'retweet_count', 'quote_count', 'reply_count', 'like_count', 'media_keys', 'created_at']\n",
    "original_data.to_json('datain/nft_original_tweets.jsonl', orient='records', index=True, lines= True)\n",
    "original_data['total'] = original_data[['retweet_count', 'quote_count','reply_count', 'like_count']].sum(axis=1)\n",
    "\n",
    "replies_data = pd.DataFrame(createRepliesDataFrame())\n",
    "replies_data.columns =['id', 'text', 'reply_author_id', 'in_reply_to_user_id', 'retweet_count', 'quote_count', 'reply_count', 'like_count', 'media_keys', 'created_at']\n",
    "replies_data.to_json('datain/nft_replies_tweets.jsonl', orient='records', index=True, lines= True)\n",
    "replies_data['total'] = replies_data[['retweet_count', 'quote_count','reply_count', 'like_count']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.reindex()\n",
    "replies_data.reindex()\n",
    "\n",
    "all_data = pd.merge(original_data, replies_data, on='in_reply_to_user_id').dropna()\n",
    "\n",
    "# replies_data['matched_id'] = pd.Series((original_data.original_author_id.isin(replies_data.reply_author_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment():\n",
    "    for i in output.index:\n",
    "        row = TextBlob(output.iloc[i]['text'])\n",
    "        if row.sentiment.polarity >= 0.7:\n",
    "                positivity = 'mostly_positive'\n",
    "        elif row.sentiment.polarity <= -0.7:\n",
    "            positivity = 'mostly_negative'\n",
    "        elif row.sentiment.polarity > -0.7 and row.sentiment.polarity < -0.4:\n",
    "            positivity = 'negative'\n",
    "        elif row.sentiment.polarity > 0.4 and row.sentiment.polarity < 0.7:\n",
    "                positivity = 'positive'\n",
    "        else:\n",
    "            positivity = 'nuetral'\n",
    "        yield  row.sentiment.polarity, row.sentiment.subjectivity, positivity\n",
    "    # with open(\"datain/cleaned.txt\", encoding='utf8') as f:\n",
    "    #     for line in f:\n",
    "    #         row = TextBlob(line)\n",
    "    #         if row.sentiment.polarity >= 0.7:\n",
    "    #             positivity = 'mostly_positive'\n",
    "    #         elif row.sentiment.polarity <= -0.7:\n",
    "    #             positivity = 'mostly_negative'\n",
    "    #         elif row.sentiment.polarity > -0.7 and row.sentiment.polarity < -0.4:\n",
    "    #             positivity = 'negative'\n",
    "    #         elif row.sentiment.polarity > 0.4 and row.sentiment.polarity < 0.7:\n",
    "    #              positivity = 'positive'\n",
    "    #         else:\n",
    "    #             positivity = 'nuetral'\n",
    "    #         yield line, row.sentiment.polarity, row.sentiment.subjectivity, positivity\n",
    "df = pd.DataFrame(getSentiment())\n",
    "df.columns =['polarity', 'subjectivity', 'positivity']\n",
    "output['polarity'] = df['polarity']\n",
    "output['subjectivity'] = df['subjectivity']\n",
    "output['positivity'] = df['positivity']\n",
    "output.to_json('datain/all_matched_author_replied_sentiment.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTagDataFrame():  \n",
    "    with open(\"datain/nft_search_tweets_sample.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                if row[\"lang\"] == 'en' and len(row.get('entities', ['hashtags'])) != 0:\n",
    "                    yield (row['id'],\n",
    "                            row['text'],\n",
    "                            row['entities']['hashtags'])\n",
    "            except KeyError:\n",
    "                pass\n",
    "#create csv\n",
    "tag_data = pd.DataFrame(createTagDataFrame())\n",
    "tag_data.columns =['id', 'text', 'hashtags']\n",
    "new_df = pd.concat([pd.DataFrame(pd.json_normalize(x)) for x in tag_data['hashtags']],ignore_index=True)\n",
    "\n",
    "tags_count = new_df['tag'].value_counts().to_dict()\n",
    "counts = pd.DataFrame.from_dict(tags_count, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_sentiment_df = pd.read_json('datain/matched_author_replied_sentiment.jsonl', lines=True)\n",
    "interactions_sentiment_df['created_at_x'] = interactions_sentiment_df['created_at_x'].astype('datetime64[ns]')\n",
    "interactions_sentiment_df['created_at_y'] = interactions_sentiment_df['created_at_y'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactions vs time\n",
    "interactions_sentiment_df.sort_values(by = 'created_at_x', ascending = False)\n",
    "plt.bar(interactions_sentiment_df['created_at_x'].dt.hour, interactions_sentiment_df['total_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node user\n",
    "#Edge from - to.\n",
    "# all_data['id_x']\n",
    "all_data = pd.read_json('datain/matched_author_replied_sentiment.jsonl', lines=True)\n",
    "# all_data['id_y']\n",
    "\n",
    "G = nx.from_pandas_edgelist(tweets.head(10000), 'author_id', 'in_reply_to_user_id') #Turn df into graph\n",
    "pos = nx.spring_layout(G) #specify layout for visual\n",
    "\n",
    "f, ax = plt.subplots(figsize=(25, 25))\n",
    "plt.style.use('ggplot')\n",
    "nodes = nx.draw_networkx_nodes(G, pos,\n",
    "                               alpha=0.8)\n",
    "nodes.set_edgecolor('k')\n",
    "nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total vs polarity corr\n",
    "lines = plt.xcorr(output['polarity'], output['total'], maxlags=9, usevlines=True)\n",
    "\n",
    "plt.title('Sentiment vs Total Interactions')\n",
    "\n",
    "plt.xlabel('Sentiment')\n",
    "\n",
    "plt.ylabel('Interactions')    \n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.axhline(0, color='red', lw=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Does this mean no correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the title\n",
    "plt.title('Correlation between Polarity and Interactions')\n",
    "  \n",
    "# plot the data\n",
    "plt.scatter(output['total'], output['polarity'])\n",
    "  \n",
    "# fits the best fitting line to the data\n",
    "plt.xcorr(output['total'], output['polarity'])\n",
    "  \n",
    "# Labelling axes\n",
    "plt.xlabel('interactions')\n",
    "plt.ylabel('polarity')\n",
    "\n",
    "output[\"total\"].corr(output[\"polarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# adds the title\n",
    "plt.title('Correlation between Polarity and Subjectivity') \n",
    "  \n",
    "# fits the best fitting line to the data\n",
    "theta = numpy.polyfit(output['subjectivity'], output['polarity'], 1)\n",
    "y_line = theta[1] + theta[0] * output['subjectivity']\n",
    "plt.scatter(output['subjectivity'], output['polarity'])  \n",
    "plt.plot(output['subjectivity'], y_line, 'r')\n",
    "# Labelling axes\n",
    "plt.xlabel('subjectivity')\n",
    "plt.ylabel('polarity')\n",
    "\n",
    "\n",
    "output[\"polarity\"].corr(output[\"subjectivity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the title\n",
    "plt.title('Correlation between Subjectivity and Interactions') \n",
    "  \n",
    "# fits the best fitting line to the data\n",
    "theta = numpy.polyfit(output['total'], output['subjectivity'], 1)\n",
    "y_line = theta[1] + theta[0] * output['total']\n",
    "plt.scatter(output['total'], output['subjectivity'])  \n",
    "plt.plot(output['total'], y_line, 'r')\n",
    "# Labelling axes\n",
    "plt.xlabel('total')\n",
    "plt.ylabel('subjectivity')\n",
    "output[\"total\"].corr(output[\"subjectivity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_vs_interactions = pd.read_json('datain/nft_top_100_cleaned_interactions_sentiment.jsonl', lines=True)\n",
    "\n",
    "df_expected = pd.concat([sentiment_vs_interactions, sentiment_vs_interactions['hashtags'].apply(pd.Series)], axis = 1).drop('hashtags', axis = 1)\n",
    "tag_array = []\n",
    "tags_array = []\n",
    "\n",
    "for j in range(100):\n",
    "    for i in range(13):\n",
    "        try:\n",
    "            tag_array.append(df_expected.iloc[j][i].get('tag'))\n",
    "        except:\n",
    "            pass\n",
    "    tags_array.append([tag_array])\n",
    "    tag_array = []\n",
    "df_expected['tags'] = tags_array\n",
    "df_expected.drop(df_expected.columns[[11,12,13,14,15,16,17,18,19,20,21,22,23]], axis=1, inplace=True)\n",
    "\n",
    "df_expected.to_json('datain/flattened_tags.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add weight to tags with most interactions\n",
    "#tags are case sensitive\n",
    "import pandas as pd\n",
    "tweets = pd.read_json('C:/Users/Administrator/Desktop/tweets.jsonl', lines=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
