{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "#nltk.download() #only for the first time running it\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import regex as re\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe on media posts USE FOR ALL DATA - Don't run now\n",
    "def createDataFrame():  \n",
    "    with open(\"datain/nft_search_tweets_sample.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                if row[\"lang\"] == 'en' and len(row.get('referenced_tweets', [])) == 0 and len(row.get('entities', ['hashtags'])) != 0:\n",
    "                    yield (row['id'],\n",
    "                            row['text'],\n",
    "                            row['entities']['hashtags'],\n",
    "                            row['public_metrics']['retweet_count'],\n",
    "                            row['public_metrics']['quote_count'],\n",
    "                            row['public_metrics']['reply_count'],\n",
    "                            row['public_metrics']['like_count'],\n",
    "                            row['attachments']['media_keys'])\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "#create csv\n",
    "df = pd.DataFrame(createDataFrame())\n",
    "df.columns =['id', 'text', 'hashtags', 'retweet_count', 'quote_count', 'reply_count', 'like_count', 'media_keys']\n",
    "df.to_json('datain/nft_tweets.jsonl', orient='records', index=True, lines= True)\n",
    "output = pd.read_json(\"datain/nft_tweets.jsonl\", lines = True)\n",
    "output['total'] = output[['retweet_count', 'quote_count','reply_count', 'like_count']].sum(axis=1)\n",
    "#sort by highest total\n",
    "output = output.sort_values(by = 'total', ascending = False)\n",
    "#top 100\n",
    "output = output.head(100)\n",
    "output.to_json('datain/nft_top_100_tweets.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "data = pd.read_json(\"datain/nft_top_100_tweets.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('rt')\n",
    "stop_words.append('nft')\n",
    "# stop_words.append('#nft')\n",
    "\n",
    "#function for cleaning a tweet (remove mentions, hashtags, links, html entities, stop words. And make sure it's only letters)\n",
    "def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        tweet = str.lower(tweet)\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "        tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|(#[A-Za-z0-9_]+)\", \" \", tweet).split()) # remove mentions and hashtags\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
    "        tweet = re.sub(\"(http\\S+|http)\", \"\", tweet, flags=re.MULTILINE) # remove links\n",
    "        tweet = re.sub('\\&\\w+', \"\", tweet) # remove html entities\n",
    "        tweet = re.sub('[^a-zA-Z# ]+', ' ', tweet) # make sure tweet is only letters\n",
    "        # stem & remove stop words\n",
    "        # tweet = ' '.join([PorterStemmer().stem(word=word) for word in tweet.split() if word not in stop_words])\n",
    "        tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "        return tweet\n",
    "\n",
    "#clean data\n",
    "for i in data.index:\n",
    "    text = data[\"text\"][i]\n",
    "    cleaned_text = clean_tweet(text)\n",
    "    cleaned_text = html.unescape(cleaned_text)\n",
    "    data[\"text\"][i] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json('datain/nft_top_100_cleaned_interactions.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nft_top_100_sentiment <- Has interactions and sentiment totals\n",
    "positivity = ''\n",
    "def getSentiment():\n",
    "    output = pd.read_json(\"datain/nft_top_100_cleaned_interactions.jsonl\", lines = True)\n",
    "    output = pd.DataFrame(output)\n",
    "    for i in output.index:\n",
    "        row = TextBlob(output.iloc[i]['text'])\n",
    "        if row.sentiment.polarity >= 0.7:\n",
    "                positivity = 'mostly_positive'\n",
    "        elif row.sentiment.polarity <= -0.7:\n",
    "            positivity = 'mostly_negative'\n",
    "        elif row.sentiment.polarity > -0.7 and row.sentiment.polarity < -0.4:\n",
    "            positivity = 'negative'\n",
    "        elif row.sentiment.polarity > 0.4 and row.sentiment.polarity < 0.7:\n",
    "                positivity = 'positive'\n",
    "        else:\n",
    "            positivity = 'nuetral'\n",
    "        yield  row.sentiment.polarity, row.sentiment.subjectivity, positivity\n",
    "        \n",
    "df = pd.DataFrame(getSentiment())\n",
    "df.columns =['polarity', 'subjectivity', 'positivity']\n",
    "output['polarity'] = df['polarity']\n",
    "output['subjectivity'] = df['subjectivity']\n",
    "output['positivity'] = df['positivity']\n",
    "output.to_json('datain/nft_top_100_cleaned_interactions_sentiment.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud of top 100 NFT's\n",
    "top_100_word_cloud = pd.read_json(\"datain/nft_top_100_cleaned_interactions_sentiment.jsonl\", lines=True)\n",
    "# Generate a word cloud image\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update((\"t\", \"co\", \"https\", \"t\", \"amp\", \"RT\"))\n",
    "wordcloud = WordCloud(stopwords=stopwords,background_color='white', max_words=1000,contour_color='#023075',contour_width=3,colormap='rainbow').generate(' '.join(top_100_word_cloud['text']))\n",
    "# create image as cloud\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# store to file\n",
    "plt.savefig(\"cloud.png\", format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment sentiment\n",
    "#### Read from nft_search_tweets_sample - need replied to with text and author\n",
    "#### Original Poster\n",
    "def createOriginalDataFrame():  \n",
    "    with open(\"datain/nft_search_tweets_sample.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                if row[\"lang\"] == 'en'and len(row.get('referenced_tweets', [])) == 0:\n",
    "                    yield (row['id'],\n",
    "                            row['text'],\n",
    "                            row['author_id'],\n",
    "                    )\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "def createRepliesDataFrame():  \n",
    "    with open(\"datain/nft_search_tweets_sample.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                if row[\"lang\"] == 'en'and len(row.get('referenced_tweets', [])) != 0:\n",
    "                    yield (row['id'],\n",
    "                            row['text'],\n",
    "                            row['author_id'],\n",
    "                            row['in_reply_to_user_id'],\n",
    "                    )\n",
    "            except KeyError:\n",
    "                pass\n",
    "#create csv\n",
    "original_data = pd.DataFrame(createOriginalDataFrame())\n",
    "original_data.columns =['id', 'text', 'in_reply_to_user_id']\n",
    "original_data.to_json('datain/nft_original_tweets.jsonl', orient='records', index=True, lines= True)\n",
    "\n",
    "replies_data = pd.DataFrame(createRepliesDataFrame())\n",
    "replies_data.columns =['id', 'text', 'reply_author_id', 'in_reply_to_user_id']\n",
    "replies_data.to_json('datain/nft_replies_tweets.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.reindex()\n",
    "replies_data.reindex()\n",
    "\n",
    "all_data = pd.merge(original_data, replies_data, on='in_reply_to_user_id').dropna()\n",
    "\n",
    "# replies_data['matched_id'] = pd.Series((original_data.original_author_id.isin(replies_data.reply_author_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment():\n",
    "    for i in all_data.index:\n",
    "        row = TextBlob(all_data.iloc[i]['text_y'])\n",
    "        if row.sentiment.polarity >= 0.7:\n",
    "                positivity = 'mostly_positive'\n",
    "        elif row.sentiment.polarity <= -0.7:\n",
    "            positivity = 'mostly_negative'\n",
    "        elif row.sentiment.polarity > -0.7 and row.sentiment.polarity < -0.4:\n",
    "            positivity = 'negative'\n",
    "        elif row.sentiment.polarity > 0.4 and row.sentiment.polarity < 0.7:\n",
    "                positivity = 'positive'\n",
    "        else:\n",
    "            positivity = 'nuetral'\n",
    "        yield  row.sentiment.polarity, row.sentiment.subjectivity, positivity\n",
    "    # with open(\"datain/cleaned.txt\", encoding='utf8') as f:\n",
    "    #     for line in f:\n",
    "    #         row = TextBlob(line)\n",
    "    #         if row.sentiment.polarity >= 0.7:\n",
    "    #             positivity = 'mostly_positive'\n",
    "    #         elif row.sentiment.polarity <= -0.7:\n",
    "    #             positivity = 'mostly_negative'\n",
    "    #         elif row.sentiment.polarity > -0.7 and row.sentiment.polarity < -0.4:\n",
    "    #             positivity = 'negative'\n",
    "    #         elif row.sentiment.polarity > 0.4 and row.sentiment.polarity < 0.7:\n",
    "    #              positivity = 'positive'\n",
    "    #         else:\n",
    "    #             positivity = 'nuetral'\n",
    "    #         yield line, row.sentiment.polarity, row.sentiment.subjectivity, positivity\n",
    "\n",
    "df = pd.DataFrame(getSentiment())\n",
    "df.columns =['polarity', 'subjectivity', 'positivity']\n",
    "all_data['polarity'] = df['polarity']\n",
    "all_data['subjectivity'] = df['subjectivity']\n",
    "all_data['positivity'] = df['positivity']\n",
    "all_data.to_json('datain/matched_author_replied_sentiment.jsonl', orient='records', index=True, lines= True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
