{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "#nltk.download() #only for the first time running it\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import regex as re\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_json('datain/nft_search_tweets_sample.jsonl')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe on media posts USE FOR ALL DATA - Don't run now\n",
    "def createDataFrame():  \n",
    "    with open(\"datain/nft_search_tweets_sample.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                if row[\"lang\"] == 'en' and len(row.get('referenced_tweets', [])) == 0 and len(row.get('entities', ['hashtags'])) != 0:\n",
    "                    yield (row['id'],\n",
    "                            row['text'],\n",
    "                            row['entities']['hashtags'],\n",
    "                            row['public_metrics']['retweet_count'],\n",
    "                            row['public_metrics']['quote_count'],\n",
    "                            row['public_metrics']['reply_count'],\n",
    "                            row['public_metrics']['like_count'],\n",
    "                            row['attachments']['media_keys'])\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "#create csv\n",
    "df = pd.DataFrame(createDataFrame())\n",
    "df.columns =['id', 'text', 'hashtags', 'retweet_count', 'quote_count', 'reply_count', 'like_count', 'media_keys']\n",
    "df.to_json('datain/nft_tweets.jsonl', orient='records', index=True, lines= True)\n",
    "output = pd.read_json(\"datain/nft_tweets.jsonl\", lines = True)\n",
    "output['total'] = output[['retweet_count', 'quote_count','reply_count', 'like_count']].sum(axis=1)\n",
    "#sort by highest total\n",
    "output = output.sort_values(by = 'total', ascending = False)\n",
    "#top 100\n",
    "output = output.head(100)\n",
    "output.to_json('datain/nft_top_100_tweets.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "data = pd.read_json(\"datain/nft_top_100_tweets.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('rt')\n",
    "stop_words.append('nft')\n",
    "# stop_words.append('#nft')\n",
    "\n",
    "#function for cleaning a tweet (remove mentions, hashtags, links, html entities, stop words. And make sure it's only letters)\n",
    "def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        tweet = str.lower(tweet)\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "        tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|(#[A-Za-z0-9_]+)\", \" \", tweet).split()) # remove mentions and hashtags\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
    "        tweet = re.sub(\"(http\\S+|http)\", \"\", tweet, flags=re.MULTILINE) # remove links\n",
    "        tweet = re.sub('\\&\\w+', \"\", tweet) # remove html entities\n",
    "        tweet = re.sub('[^a-zA-Z# ]+', ' ', tweet) # make sure tweet is only letters\n",
    "        # stem & remove stop words\n",
    "        # tweet = ' '.join([PorterStemmer().stem(word=word) for word in tweet.split() if word not in stop_words])\n",
    "        tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "        return tweet\n",
    "\n",
    "#clean data\n",
    "for i in data.index:\n",
    "    text = data[\"text\"][i]\n",
    "    cleaned_text = clean_tweet(text)\n",
    "    cleaned_text = html.unescape(cleaned_text)\n",
    "    data[\"text\"][i] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json('datain/nft_top_100_cleaned_interactions.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nft_top_100_sentiment <- Has interactions and sentiment totals\n",
    "positivity = ''\n",
    "def getSentiment():\n",
    "    output = pd.read_json(\"datain/nft_top_100_cleaned_interactions.jsonl\", lines = True)\n",
    "    output = pd.DataFrame(output)\n",
    "    for i in output.index:\n",
    "        row = TextBlob(output.iloc[i]['text'])\n",
    "        if row.sentiment.polarity >= 0.7:\n",
    "                positivity = 'mostly_positive'\n",
    "        elif row.sentiment.polarity <= -0.7:\n",
    "            positivity = 'mostly_negative'\n",
    "        elif row.sentiment.polarity > -0.7 and row.sentiment.polarity < -0.4:\n",
    "            positivity = 'negative'\n",
    "        elif row.sentiment.polarity > 0.4 and row.sentiment.polarity < 0.7:\n",
    "                positivity = 'positive'\n",
    "        else:\n",
    "            positivity = 'nuetral'\n",
    "        yield  row.sentiment.polarity, row.sentiment.subjectivity, positivity\n",
    "    # with open(\"datain/cleaned.txt\", encoding='utf8') as f:\n",
    "    #     for line in f:\n",
    "    #         row = TextBlob(line)\n",
    "    #         if row.sentiment.polarity >= 0.7:\n",
    "    #             positivity = 'mostly_positive'\n",
    "    #         elif row.sentiment.polarity <= -0.7:\n",
    "    #             positivity = 'mostly_negative'\n",
    "    #         elif row.sentiment.polarity > -0.7 and row.sentiment.polarity < -0.4:\n",
    "    #             positivity = 'negative'\n",
    "    #         elif row.sentiment.polarity > 0.4 and row.sentiment.polarity < 0.7:\n",
    "    #              positivity = 'positive'\n",
    "    #         else:\n",
    "    #             positivity = 'nuetral'\n",
    "    #         yield line, row.sentiment.polarity, row.sentiment.subjectivity, positivity\n",
    "\n",
    "df = pd.DataFrame(getSentiment())\n",
    "df.columns =['polarity', 'subjectivity', 'positivity']\n",
    "output['polarity'] = df['polarity']\n",
    "output['subjectivity'] = df['subjectivity']\n",
    "output['positivity'] = df['positivity']\n",
    "output.to_json('datain/nft_top_100_cleaned_interactions_sentiment.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-fb29227535be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Generate a word cloud image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcontour_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'#023075'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcontour_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rainbow'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_100_word_cloud\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# create image as cloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \"\"\"\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \"\"\"\n\u001b[0;32m    613\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m             \u001b[0mboolean_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_bolean_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m             \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36m_get_bolean_mask\u001b[1;34m(self, mask)\u001b[0m\n\u001b[0;32m    993\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_bolean_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m         \u001b[1;34m\"\"\"Cast to two dimensional boolean mask.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 995\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'f'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    996\u001b[0m             warnings.warn(\"mask image should be unsigned byte between 0\"\n\u001b[0;32m    997\u001b[0m                           \" and 255. Got a float array\")\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "# Word cloud of top 100 NFT's\n",
    "top_100_word_cloud = pd.read_json(\"datain/nft_top_100_cleaned_interactions_sentiment.jsonl\", lines=True)\n",
    "# Generate a word cloud image\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords=stopwords,background_color='white', max_words=1000, mask='',contour_color='#023075',contour_width=3,colormap='rainbow').generate(' '.join(top_100_word_cloud['text']))\n",
    "# create image as cloud\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "# store to file\n",
    "plt.savefig(\"cloud.png\", format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment sentiment\n",
    "#### Read from nft_search_tweets_sample - need replied to with text and author\n",
    "#### Original Poster\n",
    "def createOriginalDataFrame():  \n",
    "    with open(\"datain/nft_search_tweets_sample.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                if row[\"lang\"] == 'en'and len(row.get('referenced_tweets', [])) == 0:\n",
    "                    yield (row['id'],\n",
    "                            row['text'],\n",
    "                            row['author_id'],\n",
    "                    )\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "def createRepliesDataFrame():  \n",
    "    with open(\"datain/nft_search_tweets_sample.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                if row[\"lang\"] == 'en'and len(row.get('referenced_tweets', [])) != 0:\n",
    "                    yield (row['id'],\n",
    "                            row['text'],\n",
    "                            row['author_id'],\n",
    "                            row['in_reply_to_user_id'],\n",
    "                    )\n",
    "            except KeyError:\n",
    "                pass\n",
    "#create csv\n",
    "original_data = pd.DataFrame(createOriginalDataFrame())\n",
    "original_data.columns =['id', 'text', 'original_author_id']\n",
    "original_data.to_json('datain/nft_original_tweets.jsonl', orient='records', index=True, lines= True)\n",
    "\n",
    "replies_data = pd.DataFrame(createRepliesDataFrame())\n",
    "replies_data.columns =['id', 'text', 'reply_author_id', 'in_reply_to_user_id']\n",
    "replies_data.to_json('datain/nft_replies_tweets.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_data['matched_id'] = pd.Series((original_data.original_author_id.isin(replies_data.reply_author_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for i in replies_data.index:\n",
    "    if(replies_data.iloc[i]['matched_id'] == True):\n",
    "        data.append(replies_data.iloc[i])\n",
    "matched_df = pd.DataFrame(data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
