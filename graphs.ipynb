{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "#nltk.download() #only for the first time running it\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import regex as re\n",
    "from textblob import TextBlob\n",
    "import numpy\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe on USE FOR ALL DATA\n",
    "def createDataFrame():  \n",
    "    with open(\"C:/Users/Administrator/Desktop/tweets.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                yield (row['author_id'],\n",
    "                        row['text'],\n",
    "                        row['public_metrics']['retweet_count'],\n",
    "                        row['public_metrics']['quote_count'],\n",
    "                        row['public_metrics']['reply_count'],\n",
    "                        row['public_metrics']['like_count'])\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "#create csv\n",
    "output = pd.DataFrame(createDataFrame())\n",
    "output.columns =['author_id',  'text', 'retweet_count', 'quote_count', 'reply_count', 'like_count']\n",
    "output['total'] = output[['retweet_count', 'quote_count','reply_count', 'like_count']].sum(axis=1)\n",
    "#sort by highest total\n",
    "output = output.sort_values(by = 'total', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['total'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('rt')\n",
    "stop_words.append('nft')\n",
    "# stop_words.append('#nft')\n",
    "\n",
    "#function for cleaning a tweet (remove mentions, hashtags, links, html entities, stop words. And make sure it's only letters)\n",
    "def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        tweet = str.lower(tweet)\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "        tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|(#[A-Za-z0-9_]+)\", \" \", tweet).split()) # remove mentions and hashtags\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
    "        tweet = re.sub(\"(http\\S+|http)\", \"\", tweet, flags=re.MULTILINE) # remove links\n",
    "        tweet = re.sub('\\&\\w+', \"\", tweet) # remove html entities\n",
    "        tweet = re.sub('[^a-zA-Z# ]+', ' ', tweet) # make sure tweet is only letters\n",
    "        # stem & remove stop words\n",
    "        # tweet = ' '.join([PorterStemmer().stem(word=word) for word in tweet.split() if word not in stop_words])\n",
    "        tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "        return tweet\n",
    "\n",
    "#clean data\n",
    "for i in output.index:\n",
    "    text = output[\"text\"][i]\n",
    "    cleaned_text = clean_tweet(text)\n",
    "    cleaned_text = html.unescape(cleaned_text)\n",
    "    output[\"text\"][i] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positivity = ''\n",
    "#Textblob\n",
    "def getSentiment():\n",
    "    counter = 0\n",
    "    for i in output.index:\n",
    "        row = TextBlob(output.iloc[i]['text'])\n",
    "        counter = counter + 1\n",
    "        print('line' , counter)\n",
    "        if row.sentiment.polarity >= 0.7:\n",
    "                positivity = 'mostly_positive'\n",
    "        elif row.sentiment.polarity <= -0.7:\n",
    "            positivity = 'mostly_negative'\n",
    "        elif row.sentiment.polarity > -0.7 and row.sentiment.polarity < -0.4:\n",
    "            positivity = 'negative'\n",
    "        elif row.sentiment.polarity > 0.4 and row.sentiment.polarity < 0.7:\n",
    "                positivity = 'positive'\n",
    "        else:\n",
    "            positivity = 'nuetral'\n",
    "        yield  row.sentiment.polarity, row.sentiment.subjectivity, positivity\n",
    "        \n",
    "df = pd.DataFrame(getSentiment())\n",
    "df.columns =['polarity', 'subjectivity', 'positivity']\n",
    "output['polarity'] = df['polarity']\n",
    "output['subjectivity'] = df['subjectivity']\n",
    "output['positivity'] = df['positivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "positivity = ''\n",
    "#Textblob\n",
    "def getSentiment():\n",
    "    for i in output.index:\n",
    "        row = analyzer.polarity_scores(output.iloc[i]['text'])\n",
    "        yield  row['neg'], row['neu'], row['pos'], row['compound']\n",
    "        \n",
    "df = pd.DataFrame(getSentiment())\n",
    "df.columns =['neg', 'neu', 'pos', 'compound']\n",
    "output['Negativity'] = df['neg']\n",
    "output['Neutral'] = df['neu']\n",
    "output['Positivity'] = df['pos']\n",
    "output['Compound'] = df['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_json('graphs/tweets_vader_sentiment.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_json('graphs/tweets_replies_vader_sentiment.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_json('graphs/tweets_original_vader_sentiment.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_json('graphs/tweets_vader_sentiment.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_json('graphs/cleaned_tweets_textblob.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the title\n",
    "plt.title('Correlation between Polarity and Subjectivity') \n",
    "  \n",
    "# fits the best fitting line to the data\n",
    "theta = numpy.polyfit(output['subjectivity'], output['polarity'], 1)\n",
    "y_line = theta[1] + theta[0] * output['subjectivity']\n",
    "plt.scatter(output['subjectivity'], output['polarity'])  \n",
    "plt.plot(output['subjectivity'], y_line, 'r')\n",
    "# Labelling axes\n",
    "plt.xlabel('subjectivity')\n",
    "plt.ylabel('polarity')\n",
    "\n",
    "\n",
    "output[\"polarity\"].corr(output[\"subjectivity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_json('graphs/tweets_vader_sentiment.jsonl', lines=True)\n",
    "output['total'] = numpy.log(output.total)\n",
    "output = output.loc[output['total'] > 2]\n",
    "\n",
    "# adds the title\n",
    "plt.title('Correlation between Polarity and Total Interactions') \n",
    "\n",
    "# fits the best fitting line to the data\n",
    "theta = numpy.polyfit(output['total'], output['Compound'], 1)\n",
    "y_line = theta[1] + theta[0] * output['total']\n",
    "plt.scatter(output['total'], output['Compound'])  \n",
    "plt.plot(output['total'], y_line, 'r')\n",
    "# Labelling axes\n",
    "plt.xlabel('Total Interactions')\n",
    "plt.ylabel('Compound')\n",
    "\n",
    "\n",
    "output[\"Compound\"].corr(output[\"total\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the title\n",
    "plt.title('Correlation between Subjectivity and Total') \n",
    "  \n",
    "# fits the best fitting line to the data\n",
    "theta = numpy.polyfit(output_interactions['total'], output_interactions['subjectivity'], 1)\n",
    "y_line = theta[1] + theta[0] * output_interactions['total']\n",
    "plt.scatter(output_interactions['total'], output_interactions['subjectivity'])  \n",
    "plt.plot(output_interactions['total'], y_line, 'r')\n",
    "# Labelling axes\n",
    "plt.xlabel('total')\n",
    "plt.ylabel('subjectivity')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "output_interactions[\"subjectivity\"].corr(output_interactions[\"total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import networkx.algorithms.community as nxcom\n",
    "import karateclub \n",
    "import pandas as pd\n",
    "import json\n",
    "def createEdgeList():  \n",
    "    with open(\"C:/Users/Administrator/Desktop/tweets.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                yield (row['author_id'],\n",
    "                        row['in_reply_to_user_id'],\n",
    "                        )\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "edge_list_df = pd.DataFrame(createEdgeList())\n",
    "edge_list_df.columns =['Source', 'Target']\n",
    "edge_list_df.to_json('graphs/edgelist.jsonl', orient='records', index=True, lines= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def set_node_community(G, communities):\n",
    "    '''Add community to node attributes'''\n",
    "    for c, v_c in enumerate(communities):\n",
    "        for v in v_c:\n",
    "            # Add 1 to save 0 for external edges\n",
    "            G.nodes[v]['community'] = c + 1\n",
    "\n",
    "def set_edge_community(G):\n",
    "    '''Find internal edges and add their community to their attributes'''\n",
    "    for v, w, in G.edges:\n",
    "        if G.nodes[v]['community'] == G.nodes[w]['community']:\n",
    "            # Internal edge, mark with community\n",
    "            G.edges[v, w]['community'] = G.nodes[v]['community']\n",
    "        else:\n",
    "            # External edge, mark as 0\n",
    "            G.edges[v, w]['community'] = 0\n",
    "\n",
    "def get_color(i, r_off=1, g_off=1, b_off=1):\n",
    "    '''Assign a color to a vertex.'''\n",
    "    r0, g0, b0 = 0, 0, 0\n",
    "    n = 16\n",
    "    low, high = 0.1, 0.9\n",
    "    span = high - low\n",
    "    r = low + span * (((i + r_off) * 3) % n) / (n - 1)\n",
    "    g = low + span * (((i + g_off) * 5) % n) / (n - 1)\n",
    "    b = low + span * (((i + b_off) * 7) % n) / (n - 1)\n",
    "    return (r, g, b)     \n",
    "G = nx.from_pandas_edgelist(edge_list_df.head(1000), source='Source', target='Target')\n",
    "pos=nx.spring_layout(G)\n",
    "communities = sorted(nxcom.greedy_modularity_communities(G), key=len, reverse=True)\n",
    "\n",
    "labels = nxcom.label_propagation(G, communities)\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams.update({'figure.figsize': (15, 10)})\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# Set node and edge communities\n",
    "set_node_community(G, communities)\n",
    "set_edge_community(G)\n",
    "\n",
    "# Set community color for internal edges\n",
    "external = [(v, w) for v, w in G.edges if G.edges[v, w]['community'] == 0]\n",
    "internal = [(v, w) for v, w in G.edges if G.edges[v, w]['community'] > 0]\n",
    "internal_color = [\"black\" for e in internal]\n",
    "node_color = [get_color(G.nodes[v]['community']) for v in G.nodes]\n",
    "# external edges\n",
    "nx.draw_networkx(\n",
    "    G, \n",
    "    pos=pos, \n",
    "    node_size=0, \n",
    "    edgelist=external, \n",
    "    edge_color=\"silver\",\n",
    "    node_color=node_color,\n",
    "    alpha=0.2, \n",
    "    labels=labels)\n",
    "# internal edges\n",
    "nx.draw_networkx(\n",
    "    G, pos=pos, \n",
    "\n",
    "    edgelist=internal, \n",
    "    edge_color=internal_color,\n",
    "    node_color=node_color,\n",
    "    alpha=0.05, \n",
    "    labels=labels)\n",
    "pos = nx.spring_layout(G, k=0.1)\n",
    "plt.rcParams.update({'figure.figsize': (15, 10)})\n",
    "nx.draw_networkx(\n",
    "    G, \n",
    "    pos=pos, \n",
    "    node_size=0, \n",
    "    edge_color=\"#444444\", \n",
    "    alpha=0.05, \n",
    "    with_labels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from karateclub import DeepWalk\n",
    "\n",
    "g = nx.newman_watts_strogatz_graph(100, 20, 0.05)\n",
    "\n",
    "model = DeepWalk()\n",
    "model.fit(g)\n",
    "embedding = model.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.loc[output['Positivity'] > 0]\n",
    "output = output.loc[output['Negativity'] > 0]\n",
    "output = output.loc[output['Neutral'] > 0]\n",
    "output = output.loc[output['Compound'] > 0]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.distplot(output['Positivity'], bins=30, kde=False, \n",
    "                color='green', label='Positivity')\n",
    "sns.distplot(output['Negativity'], bins=30, kde=False, \n",
    "            color='red', label='Negativity')      \n",
    "sns.distplot(output['Neutral'], bins=30, kde=False, \n",
    "            color='grey', label='Neutral')\n",
    "sns.distplot(output['Compound'], bins=30, kde=False, \n",
    "            color='pink', label='Compound')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.legend()\n",
    "plt.title(f'Histogram of Original Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 1000 term frequency\n",
    "output = output.sort_values(by = 'total', ascending = False)\n",
    "output = output.head(10000)\n",
    "\n",
    "from collections import Counter\n",
    "results = Counter()\n",
    "output['text'].str.lower().str.split().apply(results.update)\n",
    "print(results, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('rt')\n",
    "stop_words.append('nft')\n",
    "# stop_words.append('#nft')\n",
    "counter = 0\n",
    "#function for cleaning a tweet (remove mentions, hashtags, links, html entities, stop words. And make sure it's only letters)\n",
    "def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        tweet = str.lower(tweet)\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "        tweet = ' '.join(re.sub(\"(#nft+)\", \" \", tweet).split()) # remove mentions and hashtags\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
    "        tweet = re.sub(\"(http\\S+|http)\", \"\", tweet, flags=re.MULTILINE) # remove links\n",
    "        tweet = re.sub('\\&\\w+', \"\", tweet) # remove html entities\n",
    "        # stem & remove stop words\n",
    "        # tweet = ' '.join([PorterStemmer().stem(word=word) for word in tweet.split() if word not in stop_words])\n",
    "        tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "        return tweet\n",
    "\n",
    "#clean data\n",
    "for i in output.index:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    text = output[\"text\"][i]\n",
    "    cleaned_text = clean_tweet(text)\n",
    "    cleaned_text = html.unescape(cleaned_text)\n",
    "    output[\"text\"][i] = cleaned_text\n",
    "\n",
    "output.to_json('graphs/cleaned_text.jsonl', orient='records', index=True, lines= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output = pd.read_json('graphs/cleaned_text.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def word_cloud(tweets):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"good\", \"project\", \"airdrop\", \"referrals\", \"crypto\"])\n",
    "    wordcloud = WordCloud(max_words=60, background_color='white', stopwords=stopwords,\n",
    "    random_state=2016).generate(\" \".join([tw for tw in tweets]))\n",
    "    plt.figure(figsize=(10, 5), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Twitter WordCloud\")\n",
    "\n",
    "word_cloud(output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = ' '.join(output['text'])\n",
    "tags = [re.sub(r\"(\\W+)$\", \"\", j) for j in [i for i in raw.split() if i.startswith('#')]]\n",
    "df = pd.DataFrame({\"hashtag\" : tags})\n",
    "fig, ax = plt.subplots()\n",
    "plt.xlabel('Tags')\n",
    "plt.title('Top Trending Tags')\n",
    "plt.ylabel('Count')\n",
    "df['hashtag'].value_counts().head(8).plot(ax=ax, kind='bar', figsize=(8,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (c:\\Users\\Administrator\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:301180)",
      "at w.execute (c:\\Users\\Administrator\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:300551)",
      "at w.start (c:\\Users\\Administrator\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:296215)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (c:\\Users\\Administrator\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:310950)",
      "at async t.CellExecutionQueue.start (c:\\Users\\Administrator\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:310490)"
     ]
    }
   ],
   "source": [
    "raw = ' '.join(output['text'])\n",
    "mention = [re.sub(r\"(\\W+)$\", \"\", j) for j in [i for i in raw.split() if i.startswith('@')]]\n",
    "df = pd.DataFrame({\"mentions\" : mention})\n",
    "\n",
    "from collections import Counter\n",
    "results = Counter()\n",
    "df['mentions'].str.lower().str.split().apply(results.update)\n",
    "results = dict(sorted(results.items(),key= lambda x:x[1], reverse=True))\n",
    "counts = list(results.values())[0:10]\n",
    "# [results['@cryptoultraman'], results['@cybermiles'], results['@jack_phemex'], results['@renft_protocol'], results['@asvalabofficial'], results['@kryptomonteam'],\n",
    "#         results['@nftgateio'], results['@nftstarter'], results['@babyshark_fi'], results['@dinosour'], results['@ethernaal'], \n",
    "#         results['@nft_qr_code'], results['@pegasgold'], results['@avaxstars'], results['@binancechain'], results['@projectx_nft']]\n",
    "users = list(results.keys())[0:10]\n",
    "\n",
    "plt.pie(counts,\n",
    "        labels=users,\n",
    "        startangle=10,\n",
    "        shadow='false',\n",
    "        autopct='%1.1f%%',\n",
    "        labeldistance=1.3,)\n",
    "plt.title(\"Voice Share on Twitter of Top NFT Users in %\")\n",
    "plt.legend(title=\"Users\",\n",
    "            loc=\"center left\", \n",
    "            bbox_to_anchor=(1,1,0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (c:\\Users\\Administrator\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:301180)",
      "at w.execute (c:\\Users\\Administrator\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:300551)",
      "at w.start (c:\\Users\\Administrator\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:296215)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (c:\\Users\\Administrator\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:310950)",
      "at async t.CellExecutionQueue.start (c:\\Users\\Administrator\\.vscode\\extensions\\ms-toolsai.jupyter-2021.9.1101343141\\out\\client\\extension.js:52:310490)"
     ]
    }
   ],
   "source": [
    "import igraph \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "edge_list_df = pd.read_json(\"graphs/edgelist.jsonl\", lines=True)\n",
    "g = igraph.Graph.TupleList(edge_list_df.itertuples(index=False), directed=False, weights=False, edge_attrs=\"weight\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "layout = g.layout(\"kk\")\n",
    "plot(g, layout=layout,target=ax)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe44fef87f92f48a3a32707d0df204585f471652bc0ce87358a3ce712bc24db0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
