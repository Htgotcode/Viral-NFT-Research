{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "#nltk.download() #only for the first time running it\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import regex as re\n",
    "from textblob import TextBlob\n",
    "import numpy\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "#create dataframe on USE FOR ALL DATA\n",
    "def createDataFrame():  \n",
    "    with open(\"datain/tweets.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                if len(row.get('referenced_tweets', [])) != 0:\n",
    "                    yield (row['id'],\n",
    "                            row['referenced_tweets'][0][\"id\"],\n",
    "                            row['text'],\n",
    "                            row['public_metrics']['retweet_count'],\n",
    "                            row['public_metrics']['quote_count'],\n",
    "                            row['public_metrics']['reply_count'],\n",
    "                            row['public_metrics']['like_count'])\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "#create csv\n",
    "output2 = pd.DataFrame(createDataFrame())\n",
    "output2.columns =['referenced_tweets_id', 'id', 'text',  'retweet_count', 'quote_count', 'reply_count', 'like_count']\n",
    "output2['total'] = output2[['retweet_count', 'quote_count','reply_count', 'like_count']].sum(axis=1)\n",
    "#sort by highest total\n",
    "output2 = output2.sort_values(by = 'total', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['text'].head(6))\n",
    "print(output['total'].head(6))\n",
    "\n",
    "pd.options.display.max_colwidth = 10000\n",
    "output['hashtags'].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('rt')\n",
    "stop_words.append('nft')\n",
    "# stop_words.append('#nft')\n",
    "\n",
    "#function for cleaning a tweet (remove mentions, hashtags, links, html entities, stop words. And make sure it's only letters)\n",
    "def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        tweet = str.lower(tweet)\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|(#[A-Za-z0-9_]+)\", \" \", tweet).split()) # remove mentions and hashtags\n",
    "        # # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
    "        # tweet = re.sub(\"(http\\S+|http)\", \"\", tweet, flags=re.MULTILINE) # remove links\n",
    "        # tweet = re.sub('\\&\\w+', \"\", tweet) # remove html entities\n",
    "        # tweet = re.sub('[^a-zA-Z# ]+', ' ', tweet) # make sure tweet is only letters\n",
    "        # # stem & remove stop words\n",
    "        # # tweet = ' '.join([PorterStemmer().stem(word=word) for word in tweet.split() if word not in stop_words])\n",
    "        # tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "        return tweet\n",
    "\n",
    "#clean data\n",
    "for i in output.index:\n",
    "    text = output[\"text\"][i]\n",
    "    cleaned_text = clean_tweet(text)\n",
    "    cleaned_text = html.unescape(cleaned_text)\n",
    "    output[\"text\"][i] = cleaned_text\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positivity = ''\n",
    "#Textblob\n",
    "def getSentiment():\n",
    "    counter = 0\n",
    "    for i in output.index:\n",
    "        row = TextBlob(output.iloc[i]['text'])\n",
    "        counter = counter + 1\n",
    "        print('line' , counter)\n",
    "        if row.sentiment.polarity >= 0.7:\n",
    "                positivity = 'mostly_positive'\n",
    "        elif row.sentiment.polarity <= -0.7:\n",
    "            positivity = 'mostly_negative'\n",
    "        elif row.sentiment.polarity > -0.7 and row.sentiment.polarity < -0.4:\n",
    "            positivity = 'negative'\n",
    "        elif row.sentiment.polarity > 0.4 and row.sentiment.polarity < 0.7:\n",
    "                positivity = 'positive'\n",
    "        else:\n",
    "            positivity = 'nuetral'\n",
    "        yield  row.sentiment.polarity, row.sentiment.subjectivity, positivity\n",
    "        \n",
    "df = pd.DataFrame(getSentiment())\n",
    "df.columns =['polarity', 'subjectivity', 'positivity']\n",
    "output['polarity'] = df['polarity']\n",
    "output['subjectivity'] = df['subjectivity']\n",
    "output['positivity'] = df['positivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "positivity = ''\n",
    "#Textblob\n",
    "def getSentiment():\n",
    "    for i in output.index:\n",
    "        row = analyzer.polarity_scores(output.iloc[i]['text'])\n",
    "        yield  row['neg'], row['neu'], row['pos'], row['compound']\n",
    "        \n",
    "df = pd.DataFrame(getSentiment())\n",
    "df.columns =['neg', 'neu', 'pos', 'compound']\n",
    "output['Negativity'] = df['neg']\n",
    "output['Neutral'] = df['neu']\n",
    "output['Positivity'] = df['pos']\n",
    "output['Compound'] = df['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_json('graphs/lowercase.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = pd.read_json('C:/Users/Administrator/Desktop/tweets.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_json('graphs/tweets_replies_vader_sentiment.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_json('graphs/tweets_original_vader_sentiment.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output = pd.read_json('datain/tweets.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_json('graphs/cleaned_tweets_textblob.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the title\n",
    "plt.title('Correlation between Polarity and Subjectivity') \n",
    "  \n",
    "# fits the best fitting line to the data\n",
    "theta = numpy.polyfit(output['subjectivity'], output['polarity'], 1)\n",
    "y_line = theta[1] + theta[0] * output['subjectivity']\n",
    "plt.scatter(output['subjectivity'], output['polarity'])  \n",
    "plt.plot(output['subjectivity'], y_line, 'r')\n",
    "# Labelling axes\n",
    "plt.xlabel('subjectivity')\n",
    "plt.ylabel('polarity')\n",
    "\n",
    "\n",
    "output[\"polarity\"].corr(output[\"subjectivity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_json('graphs/tweets_vader_sentiment.jsonl', lines=True)\n",
    "output['total'] = numpy.log(output.total)\n",
    "output = output.loc[output['total'] > 2]\n",
    "\n",
    "# adds the title\n",
    "plt.title('Correlation between Polarity and Total Interactions') \n",
    "\n",
    "# fits the best fitting line to the data\n",
    "theta = numpy.polyfit(output['total'], output['Compound'], 1)\n",
    "y_line = theta[1] + theta[0] * output['total']\n",
    "plt.scatter(output['total'], output['Compound'])  \n",
    "plt.plot(output['total'], y_line, 'r')\n",
    "# Labelling axes\n",
    "plt.xlabel('Total Interactions')\n",
    "plt.ylabel('Compound')\n",
    "\n",
    "\n",
    "output[\"Compound\"].corr(output[\"total\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_json('graphs/cleaned_tweets_textblob.jsonl', lines=True)\n",
    "output['total'] = numpy.log(output.total)\n",
    "output = output.loc[output['total'] > 2]\n",
    "\n",
    "# adds the title\n",
    "plt.title('Correlation between Subjectivity and Total Interactions') \n",
    "\n",
    "# fits the best fitting line to the data\n",
    "theta = numpy.polyfit(output['total'], output['subjectivity'], 1)\n",
    "y_line = theta[1] + theta[0] * output['total']\n",
    "plt.scatter(output['total'], output['subjectivity'])  \n",
    "plt.plot(output['total'], y_line, 'r')\n",
    "# Labelling axes\n",
    "plt.xlabel('Total Interactions')\n",
    "plt.ylabel('Subjectivity')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "output[\"subjectivity\"].corr(output[\"total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import networkx.algorithms.community as nxcom\n",
    "import karateclub \n",
    "import pandas as pd\n",
    "import json\n",
    "def createEdgeList():  \n",
    "    with open(\"C:/Users/Administrator/Desktop/tweets.jsonl\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            try:\n",
    "                yield (row['author_id'],\n",
    "                        row['in_reply_to_user_id'],\n",
    "                        )\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "edge_list_df = pd.DataFrame(createEdgeList())\n",
    "edge_list_df.columns =['source_id', 'target_id']\n",
    "edge_list_df.to_('graphs/edgelist.net', orient='records', index=True, lines= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def set_node_community(G, communities):\n",
    "    '''Add community to node attributes'''\n",
    "    for c, v_c in enumerate(communities):\n",
    "        for v in v_c:\n",
    "            # Add 1 to save 0 for external edges\n",
    "            G.nodes[v]['community'] = c + 1\n",
    "\n",
    "def set_edge_community(G):\n",
    "    '''Find internal edges and add their community to their attributes'''\n",
    "    for v, w, in G.edges:\n",
    "        if G.nodes[v]['community'] == G.nodes[w]['community']:\n",
    "            # Internal edge, mark with community\n",
    "            G.edges[v, w]['community'] = G.nodes[v]['community']\n",
    "        else:\n",
    "            # External edge, mark as 0\n",
    "            G.edges[v, w]['community'] = 0\n",
    "\n",
    "def get_color(i, r_off=1, g_off=1, b_off=1):\n",
    "    '''Assign a color to a vertex.'''\n",
    "    r0, g0, b0 = 0, 0, 0\n",
    "    n = 16\n",
    "    low, high = 0.1, 0.9\n",
    "    span = high - low\n",
    "    r = low + span * (((i + r_off) * 3) % n) / (n - 1)\n",
    "    g = low + span * (((i + g_off) * 5) % n) / (n - 1)\n",
    "    b = low + span * (((i + b_off) * 7) % n) / (n - 1)\n",
    "    return (r, g, b)     \n",
    "G = nx.from_pandas_edgelist(edge_list_df.head(1000), source='Source', target='Target')\n",
    "pos=nx.spring_layout(G)\n",
    "communities = sorted(nxcom.greedy_modularity_communities(G), key=len, reverse=True)\n",
    "\n",
    "labels = nxcom.label_propagation(G, communities)\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams.update({'figure.figsize': (15, 10)})\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# Set node and edge communities\n",
    "set_node_community(G, communities)\n",
    "set_edge_community(G)\n",
    "\n",
    "# Set community color for internal edges\n",
    "external = [(v, w) for v, w in G.edges if G.edges[v, w]['community'] == 0]\n",
    "internal = [(v, w) for v, w in G.edges if G.edges[v, w]['community'] > 0]\n",
    "internal_color = [\"black\" for e in internal]\n",
    "node_color = [get_color(G.nodes[v]['community']) for v in G.nodes]\n",
    "# external edges\n",
    "nx.draw_networkx(\n",
    "    G, \n",
    "    pos=pos, \n",
    "    node_size=0, \n",
    "    edgelist=external, \n",
    "    edge_color=\"silver\",\n",
    "    node_color=node_color,\n",
    "    alpha=0.2, VA\n",
    "    labels=labels)\n",
    "# internal edges\n",
    "nx.draw_networkx(\n",
    "    G, pos=pos, \n",
    "\n",
    "    edgelist=internal, \n",
    "    edge_color=internal_color,\n",
    "    node_color=node_color,\n",
    "    alpha=0.05, \n",
    "    labels=labels)\n",
    "pos = nx.spring_layout(G, k=0.1)\n",
    "plt.rcParams.update({'figure.figsize': (15, 10)})\n",
    "nx.draw_networkx(\n",
    "    G, \n",
    "    pos=pos, \n",
    "    node_size=0, \n",
    "    edge_color=\"#444444\", \n",
    "    alpha=0.05, \n",
    "    with_labels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.loc[output['Positivity'] > 0]\n",
    "output = output.loc[output['Negativity'] > 0]\n",
    "output = output.loc[output['Neutral'] > 0]\n",
    "output = output.loc[output['Compound'] > 0]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.distplot(output['Positivity'], bins=30, kde=False, \n",
    "                color='green', label='Positivity')\n",
    "sns.distplot(output['Negativity'], bins=30, kde=False, \n",
    "            color='red', label='Negativity')      \n",
    "sns.distplot(output['Neutral'], bins=30, kde=False, \n",
    "            color='grey', label='Neutral')\n",
    "sns.distplot(output['Compound'], bins=30, kde=False, \n",
    "            color='pink', label='Compound')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.legend()\n",
    "plt.title(f'Histogram of Original Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 1000 term frequency\n",
    "output = output.sort_values(by = 'total', ascending = False)\n",
    "output = output.head(10000)\n",
    "\n",
    "from collections import Counter\n",
    "results = Counter()\n",
    "output['text'].str.lower().str.split().apply(results.update)\n",
    "print(results, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('rt')\n",
    "stop_words.append('nft')\n",
    "# stop_words.append('#nft')\n",
    "counter = 0\n",
    "#function for cleaning a tweet (remove mentions, hashtags, links, html entities, stop words. And make sure it's only letters)\n",
    "def clean_tweet(tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        tweet = str.lower(tweet)\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "        tweet = ' '.join(re.sub(\"(#nft+)\", \" \", tweet).split()) # remove mentions and hashtags\n",
    "        # tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)\", \" \", tweet).split())\n",
    "        tweet = re.sub(\"(http\\S+|http)\", \"\", tweet, flags=re.MULTILINE) # remove links\n",
    "        tweet = re.sub('\\&\\w+', \"\", tweet) # remove html entities\n",
    "        # stem & remove stop words\n",
    "        # tweet = ' '.join([PorterStemmer().stem(word=word) for word in tweet.split() if word not in stop_words])\n",
    "        tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "        return tweet\n",
    "\n",
    "#clean data\n",
    "for i in output.index:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    text = output[\"text\"][i]\n",
    "    cleaned_text = clean_tweet(text)\n",
    "    cleaned_text = html.unescape(cleaned_text)\n",
    "    output[\"text\"][i] = cleaned_text\n",
    "\n",
    "output.to_json('graphs/cleaned_text.jsonl', orient='records', index=True, lines= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output = pd.read_json('graphs/cleaned_text.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def word_cloud(tweets):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"good\", \"project\", \"airdrop\", \"referrals\", \"crypto\"])\n",
    "    wordcloud = WordCloud(max_words=60, background_color='white', stopwords=stopwords,\n",
    "    random_state=2016).generate(\" \".join([tw for tw in tweets]))\n",
    "    plt.figure(figsize=(10, 5), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Twitter WordCloud\")\n",
    "\n",
    "word_cloud(output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = ' '.join(output['text'])\n",
    "tags = [re.sub(r\"(\\W+)$\", \"\", j) for j in [i for i in raw.split() if i.startswith('#')]]\n",
    "df = pd.DataFrame({\"hashtag\" : tags})\n",
    "fig, ax = plt.subplots()\n",
    "plt.xlabel('Tags')\n",
    "plt.title('Top Trending Tags')\n",
    "plt.ylabel('Count')\n",
    "df['hashtag'].value_counts().head(8).plot(ax=ax, kind='bar', figsize=(8,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = ' '.join(output['text'])\n",
    "mention = [re.sub(r\"(\\W+)$\", \"\", j) for j in [i for i in raw.split() if i.startswith('@')]]\n",
    "df = pd.DataFrame({\"mentions\" : mention})\n",
    "\n",
    "from collections import Counter\n",
    "results = Counter()\n",
    "df['mentions'].str.lower().str.split().apply(results.update)\n",
    "results = results.most_common(9)\n",
    "# results = dict(sorted(results.items(), key= lambda x:x[1], reverse=True))\n",
    "\n",
    "counter = 0\n",
    "counts.clear()\n",
    "users.clear()\n",
    "for i in results:\n",
    "    counter += i[1]\n",
    "print(counter)\n",
    "for i in results:\n",
    "    counts.append(i[1] / counter * 100)\n",
    "    users.append(i[0])\n",
    "print(results)\n",
    "\n",
    "\n",
    "# counts = list(results.values())[0:10]\n",
    "# # [results['@cryptoultraman'], results['@cybermiles'], results['@jack_phemex'], results['@renft_protocol'], results['@asvalabofficial'], results['@kryptomonteam'],\n",
    "# #         results['@nftgateio'], results['@nftstarter'], results['@babyshark_fi'], results['@dinosour'], results['@ethernaal'], \n",
    "# #         results['@nft_qr_code'], results['@pegasgold'], results['@avaxstars'], results['@binancechain'], results['@projectx_nft']]\n",
    "# users = list(results.keys())[0:10]\n",
    "\n",
    "plt.bar(users, counts)\n",
    "plt.title(\"\")\n",
    "plt.ylabel(\"Voice Share (%)\")\n",
    "plt.xlabel(\"User Screen Name\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results\n",
    "s = sum(results.values())\n",
    "for k, v in results.items():\n",
    "    pct = v * 100.0 / s\n",
    "    print(k, pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from igraph import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "edge_list_df = pd.read_json(\"graphs/edgelist.jsonl\", lines=True)\n",
    "\n",
    "g = igraph.Graph.DataFrame(edge_list_df)\n",
    "\n",
    "print(g.edge_betweenness())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_json('graphs/tweets_vader_sentiment.jsonl', lines=True)\n",
    "#Top 10000 bottom 10000 sentiment\n",
    "output = output.sort_values(by = 'total', ascending = False)\n",
    "top100 = output.head(100000)\n",
    "output = output.sort_values(by = 'total', ascending = True)\n",
    "button100 = output.head(100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100 = top100.loc[output['Positivity'] > 0]\n",
    "top100 = top100.loc[output['Negativity'] > 0]\n",
    "top100 = top100.loc[output['Neutral'] > 0]\n",
    "top100 = top100.loc[output['Compound'] > 0]\n",
    "button100 = button100.loc[output['Positivity'] > 0]\n",
    "button100 = button100.loc[output['Negativity'] > 0]\n",
    "button100 = button100.loc[output['Neutral'] > 0]\n",
    "button100 = button100.loc[output['Compound'] > 0]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.distplot(top100['Positivity'], bins=30, kde=False, \n",
    "                color='green', label='Positivity')\n",
    "sns.distplot(top100['Negativity'], bins=30, kde=False, \n",
    "            color='red', label='Negativity')      \n",
    "sns.distplot(top100['Neutral'], bins=30, kde=False, \n",
    "            color='grey', label='Neutral')\n",
    "sns.distplot(top100['Compound'], bins=30, kde=False, \n",
    "            color='pink', label='Compound')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.legend()\n",
    "plt.title(f'Top 100 Interactions Histogram of Sentiment')\n",
    "\n",
    "button100 = button100.loc[output['Positivity'] > 0]\n",
    "button100 = button100.loc[output['Negativity'] > 0]\n",
    "button100 = button100.loc[output['Neutral'] > 0]\n",
    "button100 = button100.loc[output['Compound'] > 0]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.distplot(button100['Positivity'], bins=30, kde=False, \n",
    "                color='green', label='Positivity')\n",
    "sns.distplot(button100['Negativity'], bins=30, kde=False, \n",
    "            color='red', label='Negativity')      \n",
    "sns.distplot(button100['Neutral'], bins=30, kde=False, \n",
    "            color='grey', label='Neutral')\n",
    "sns.distplot(button100['Compound'], bins=30, kde=False, \n",
    "            color='pink', label='Compound')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.legend()\n",
    "plt.title(f'Bottom 100 Interactions Histogram of Sentiment')\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.distplot(top100['Compound'], bins=30, kde=False, \n",
    "                color='green', label='Top Compound Sentiment')\n",
    "sns.distplot(button100['Compound'], bins=30, kde=False, \n",
    "            color='red', label='Bottom Compound Sentiment')      \n",
    "plt.ylabel('Tweet Count')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.legend()\n",
    "plt.title(f'Bottom & Top 2500 Interactions Histogram of Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.sort_values(by = 'total', ascending = False)\n",
    "graph = output.head(2)\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(graph['author_id'], graph['total'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kneed\n",
    "from kneed import KneeLocator, DataGenerator as dg\n",
    "\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "y = [-25186946, -24615004, -24430161, -24228294, -24095226, -23997376, -23891756, -23876779, -23797978, -23690535, -23650680, -23645136, -23567772, -23524427, -23507163, -23478556, -23417238, -23406857, -23394129, -23354058, -23325430, -23290953, -23271372, -23256990]\n",
    "kl = KneeLocator(x, y, curve=\"concave\", direction=\"increasing\")\n",
    "kl.plot_knee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "edgelist = pd.read_json(\"graphs/edgelist.jsonl\", lines=True)\n",
    "edgelist.set_index('source_id', inplace=True)\n",
    "edgelist.to_string(\"graphs/network.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infomap\n",
    "from infomap import Infomap\n",
    "\n",
    "im = Infomap(silent=True, num_trials=10, input_format='link-list')\n",
    "im.read_file(\"graphs/network.net\")\n",
    "im.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output = pd.read_json(\"graphs/cleaned_tweets_textblob.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "rslt_df = output[output['text'].str.contains('giveaway')]\n",
    "rslt_df = rslt_df.append(output[output['text'].str.contains('lottery')])\n",
    "\n",
    "# for i in output.index:\n",
    "#     if(output.iloc[i]['text'].lower().__contains__('giveaway') or output.iloc[i]['text'].lower().__contains__('lottery')):\n",
    "#         giveaway_df = output.iloc[i]\n",
    "rslt_df['total_row'] = rslt_df['total'].sum(axis=0)        \n",
    "output['total_row'] = output['total'].sum(axis=0)     \n",
    "percentage = len(rslt_df) / len(output) * 100\n",
    "# 1.8710321908074146\n",
    "# 19.305024482142898\n",
    "# 447469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "86384 / 447469 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vader = pd.read_json('graphs/tweets_vader_sentiment.jsonl', lines=True)\n",
    "replies = pd.read_json('datain/nft_tweets.jsonl', lines=True)\n",
    "replies = pd.DataFrame(replies[['author_id','in_reply_to_user_id', 'text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = pd.merge(output, output2, on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER2\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Textblob\n",
    "def getSentiment():\n",
    "    for i in vader.index:\n",
    "        row = analyzer.polarity_scores(vader.iloc[i]['text_x'])\n",
    "        yield  row['compound']\n",
    "        \n",
    "df = pd.DataFrame(getSentiment())\n",
    "df.columns =['compound']\n",
    "vader['compound_x'] = df['compound']\n",
    "\n",
    "#Textblob\n",
    "def getSentiment():\n",
    "    for i in vader.index:\n",
    "        row = analyzer.polarity_scores(vader.iloc[i]['text_y'])\n",
    "        yield  row['compound']\n",
    "        \n",
    "df2 = pd.DataFrame(getSentiment())\n",
    "df2.columns =['compound']\n",
    "vader['compound_y'] = df2['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = vader[['id', 'text_x', 'total_x', 'referenced_tweets_id', 'text_y', 'total_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader.to_json('graphs/sentiment_total_from_replies.jsonl', orient='records', index=True, lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vader = pd.read_json('graphs/sentiment_original_replies.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = pd.unique(vader['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0 \n",
    "divider= 0\n",
    "totals = []\n",
    "for j in unique_ids:\n",
    "    \n",
    "    divider = 0\n",
    "    total = 0 \n",
    "    for i in vader.index:\n",
    "        if(vader.iloc[i]['id'] == j):\n",
    "            divider += 1\n",
    "            total += vader.iloc[i]['compound_y'] \n",
    "    totals.append(total / divider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(unique_ids, totals)\n",
    "new_df = new_df.reset_index()\n",
    "new_df.columns =['totals','id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = pd.merge(vader, new_df, on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = vader.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe44fef87f92f48a3a32707d0df204585f471652bc0ce87358a3ce712bc24db0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
